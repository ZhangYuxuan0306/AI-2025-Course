"""å®Œæ•´çš„RAGç³»ç»Ÿè¯„ä¼°è„šæœ¬"""
import json
from pathlib import Path
from datetime import datetime
from loguru import logger
import pandas as pd
import config
from src.document_loader import DocumentProcessor
from src.vector_store import VectorStoreManager
from src.retriever import RetrieverManager
from src.generator import AnswerGenerator, RAGPipeline
from src.evaluation import RAGEvaluator, FailureAnalyzer

# é…ç½®æ—¥å¿—
logger.add(
    config.LOGS_DIR / "evaluation.log",
    rotation="500 MB",
    level=config.LOG_LEVEL
)


def load_test_cases(test_file: str = None) -> list:
    """åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
    if test_file and Path(test_file).exists():
        with open(test_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    # é»˜è®¤æµ‹è¯•ç”¨ä¾‹
    return [
        {
            "question": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "expected_keywords": ["äººå·¥æ™ºèƒ½", "AI", "è®¡ç®—æœº", "æ™ºèƒ½"],
            "ground_truth": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯..."
        },
        {
            "question": "æœºå™¨å­¦ä¹ æœ‰å“ªäº›åº”ç”¨ï¼Ÿ",
            "expected_keywords": ["æœºå™¨å­¦ä¹ ", "åº”ç”¨", "å›¾åƒè¯†åˆ«", "è‡ªç„¶è¯­è¨€"],
            "ground_truth": "æœºå™¨å­¦ä¹ åº”ç”¨å¹¿æ³›ï¼ŒåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰..."
        },
        {
            "question": "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
            "expected_keywords": ["æ·±åº¦å­¦ä¹ ", "æœºå™¨å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "åŒºåˆ«"],
            "ground_truth": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œ..."
        }
    ]


def main():
    """ä¸»è¯„ä¼°æµç¨‹"""
    logger.info("=" * 70)
    logger.info("RAGç³»ç»Ÿå®Œæ•´è¯„ä¼°æµç¨‹")
    logger.info("=" * 70)
    
    # 1. åˆå§‹åŒ–ç³»ç»Ÿ
    logger.info("\næ­¥éª¤1: åˆå§‹åŒ–RAGç³»ç»Ÿ...")
    
    processor = DocumentProcessor()
    chunks = processor.process_documents(config.DOCUMENTS_PATH)
    
    if not chunks:
        logger.error("æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼")
        return
    
    # åˆ›å»º/åŠ è½½å‘é‡ç´¢å¼•
    vs_manager = VectorStoreManager()
    index_path = Path(config.VECTOR_DB_PATH) / "evaluation"
    
    if index_path.exists():
        logger.info("åŠ è½½å·²æœ‰ç´¢å¼•...")
        vs_manager.load("evaluation")
    else:
        logger.info("åˆ›å»ºæ–°ç´¢å¼•...")
        vs_manager.create_vectorstore(chunks)
        vs_manager.save("evaluation")
    
    # åˆå§‹åŒ–æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨
    retriever_manager = RetrieverManager(vs_manager)
    retriever_manager.setup_bm25(chunks)
    
    generator = AnswerGenerator()
    rag_pipeline = RAGPipeline(retriever_manager, generator)
    
    logger.info("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    # 2. åŠ è½½æµ‹è¯•ç”¨ä¾‹
    logger.info("\næ­¥éª¤2: åŠ è½½æµ‹è¯•ç”¨ä¾‹...")
    test_cases = load_test_cases()
    logger.info(f"åŠ è½½äº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    
    # 3. æ‰§è¡ŒæŸ¥è¯¢å¹¶æ”¶é›†ç»“æœ
    logger.info("\næ­¥éª¤3: æ‰§è¡ŒæŸ¥è¯¢...")
    results = {
        'faiss': [],
        'bm25': [],
        'hybrid': []
    }
    
    for retriever_type in ['faiss', 'bm25', 'hybrid']:
        logger.info(f"\nä½¿ç”¨ {retriever_type} æ£€ç´¢å™¨...")
        
        for i, test_case in enumerate(test_cases, 1):
            question = test_case['question']
            logger.info(f"  æŸ¥è¯¢ {i}/{len(test_cases)}: {question}")
            
            try:
                result = rag_pipeline.query(
                    question,
                    retriever_type=retriever_type
                )
                results[retriever_type].append(result)
            except Exception as e:
                logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
                continue
    
    # 4. æ€§èƒ½è¯„ä¼°ï¼šå»¶è¿Ÿå’Œååé‡
    logger.info("\næ­¥éª¤4: æ€§èƒ½è¯„ä¼°...")
    evaluator = RAGEvaluator()
    
    test_questions = [case['question'] for case in test_cases]
    performance_metrics = evaluator.measure_performance(
        rag_pipeline,
        test_questions,
        retriever_types=['faiss', 'bm25', 'hybrid']
    )
    
    logger.info("âœ… æ€§èƒ½è¯„ä¼°å®Œæˆ")
    
    # 5. æ£€ç´¢æŒ‡æ ‡è¯„ä¼°
    logger.info("\næ­¥éª¤5: æ£€ç´¢æŒ‡æ ‡è¯„ä¼°...")
    
    retrieval_metrics = {}
    for retriever_type in ['faiss', 'bm25', 'hybrid']:
        retrieved_docs = []
        relevant_docs = []
        
        for result in results[retriever_type]:
            # æå–æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹
            docs = [source['content'] for source in result.get('sources', [])]
            retrieved_docs.append(docs)
            
            # ç®€å•å‡è®¾ï¼šå¦‚æœç­”æ¡ˆåŒ…å«é¢„æœŸå…³é”®è¯ï¼Œåˆ™è§†ä¸ºç›¸å…³
            # å®é™…åº”ç”¨ä¸­éœ€è¦äººå·¥æ ‡æ³¨
            relevant_docs.append(docs[:3])  # å‡è®¾å‰3ä¸ªç›¸å…³
        
        if retrieved_docs and relevant_docs:
            metrics = evaluator.calculate_retrieval_metrics(
                retrieved_docs,
                relevant_docs
            )
            retrieval_metrics[retriever_type] = metrics
    
    logger.info("âœ… æ£€ç´¢æŒ‡æ ‡è¯„ä¼°å®Œæˆ")
    
    # 6. RAGASè¯„ä¼°ï¼ˆå¯é€‰ï¼Œéœ€è¦é…ç½®LLMï¼‰
    logger.info("\næ­¥éª¤6: RAGASè¯„ä¼°...")
    
    ragas_results = {}
    try:
        for retriever_type in ['faiss']:  # ç¤ºä¾‹åªè¯„ä¼°FAISS
            questions = []
            answers = []
            contexts = []
            
            for result in results[retriever_type]:
                questions.append(result['question'])
                answers.append(result['answer'])
                
                # æå–ä¸Šä¸‹æ–‡
                context = [
                    source['content'] 
                    for source in result.get('sources', [])
                ]
                contexts.append(context)
            
            if questions and answers and contexts:
                dataset = evaluator.prepare_evaluation_dataset(
                    questions,
                    answers,
                    contexts
                )
                
                # æ³¨æ„ï¼šRAGASè¯„ä¼°éœ€è¦é…ç½®æœ‰æ•ˆçš„LLM API
                # ragas_result = evaluator.evaluate_rag_system(dataset)
                # ragas_results[retriever_type] = ragas_result
                
                logger.info(f"å‡†å¤‡äº† {len(dataset)} ä¸ªæ ·æœ¬ç”¨äºRAGASè¯„ä¼°")
                logger.info("æç¤º: RAGASè¯„ä¼°éœ€è¦é…ç½®æœ‰æ•ˆçš„LLM API")
    
    except Exception as e:
        logger.warning(f"RAGASè¯„ä¼°è·³è¿‡: {e}")
    
    # 7. å¤±è´¥æ¡ˆä¾‹åˆ†æ
    logger.info("\næ­¥éª¤7: å¤±è´¥æ¡ˆä¾‹åˆ†æ...")
    
    failure_analyzer = FailureAnalyzer()
    failure_cases = []
    
    for retriever_type in ['faiss']:
        for i, result in enumerate(results[retriever_type]):
            test_case = test_cases[i]
            
            failure_case = {
                'question': result['question'],
                'generated_answer': result['answer'],
                'expected_answer': test_case.get('ground_truth', ''),
                'retrieved_docs': [
                    source['content'] 
                    for source in result.get('sources', [])
                ],
                'relevant_docs': [],  # éœ€è¦äººå·¥æ ‡æ³¨
                'scores': [
                    source.get('score', 0) 
                    for source in result.get('sources', [])
                ]
            }
            failure_cases.append(failure_case)
    
    failure_analysis = failure_analyzer.batch_analyze_failures(failure_cases)
    logger.info("âœ… å¤±è´¥æ¡ˆä¾‹åˆ†æå®Œæˆ")
    
    # 8. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    logger.info("\næ­¥éª¤8: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = config.RESULTS_DIR / f"evaluation_results_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            'performance_metrics': performance_metrics,
            'retrieval_metrics': retrieval_metrics,
            'ragas_results': ragas_results,
            'failure_analysis': failure_analysis
        }, f, ensure_ascii=False, indent=2)
    
    logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_file}")
    
    # ç”ŸæˆCSVæŠ¥å‘Š
    report_file = config.RESULTS_DIR / f"evaluation_report_{timestamp}.csv"
    
    report_data = []
    
    # æ€§èƒ½æŒ‡æ ‡
    for retriever, metrics in performance_metrics.items():
        for metric, value in metrics.items():
            report_data.append({
                'Category': 'Performance',
                'Retriever': retriever,
                'Metric': metric,
                'Value': value
            })
    
    # æ£€ç´¢æŒ‡æ ‡
    for retriever, metrics in retrieval_metrics.items():
        for metric, value in metrics.items():
            report_data.append({
                'Category': 'Retrieval',
                'Retriever': retriever,
                'Metric': metric,
                'Value': value
            })
    
    df = pd.DataFrame(report_data)
    df.to_csv(report_file, index=False, encoding='utf-8-sig')
    
    logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    # ç”Ÿæˆå¤±è´¥æ¡ˆä¾‹æŠ¥å‘Š
    failure_report_file = config.RESULTS_DIR / f"failure_analysis_{timestamp}.csv"
    failure_df = failure_analyzer.generate_failure_report(
        failure_analysis,
        str(failure_report_file)
    )
    
    # 9. æ‰“å°æ‘˜è¦
    logger.info("\n" + "=" * 70)
    logger.info("è¯„ä¼°æ‘˜è¦")
    logger.info("=" * 70)
    
    print("\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
    for retriever, metrics in performance_metrics.items():
        print(f"\n  {retriever.upper()}:")
        print(f"    å¹³å‡å»¶è¿Ÿ: {metrics['avg_latency']:.3f}ç§’")
        print(f"    ååé‡: {metrics['throughput']:.2f} queries/sec")
    
    print("\nğŸ” æ£€ç´¢æŒ‡æ ‡:")
    for retriever, metrics in retrieval_metrics.items():
        print(f"\n  {retriever.upper()}:")
        for metric, value in metrics.items():
            print(f"    {metric}: {value:.4f}")
    
    print("\nâŒ å¤±è´¥æ¡ˆä¾‹ç»Ÿè®¡:")
    for error_type, count in failure_analysis['error_statistics'].items():
        print(f"  {error_type}: {count}")
    
    logger.info("\nâœ… è¯„ä¼°å®Œæˆï¼")
    logger.info(f"ç»“æœä¿å­˜åœ¨: {config.RESULTS_DIR}")


if __name__ == "__main__":
    main()

