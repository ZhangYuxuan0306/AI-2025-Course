# RAGé—®ç­”ç³»ç»Ÿä½¿ç”¨æŒ‡å—

## ç›®å½•
1. [å¿«é€Ÿå…¥é—¨](#å¿«é€Ÿå…¥é—¨)
2. [è¯¦ç»†é…ç½®](#è¯¦ç»†é…ç½®)
3. [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
4. [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
5. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## å¿«é€Ÿå…¥é—¨

### ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡

#### Windowsç”¨æˆ·
```bash
# åŒå‡»è¿è¡Œæˆ–åœ¨å‘½ä»¤è¡Œæ‰§è¡Œ
start.bat
```

#### Linux/Macç”¨æˆ·
```bash
# åœ¨ç»ˆç«¯æ‰§è¡Œ
chmod +x start.sh
./start.sh
```

### ç¬¬äºŒæ­¥ï¼šå‡†å¤‡æ–‡æ¡£

å°†æ‚¨çš„æ–‡æ¡£æ”¾å…¥ `data/documents/` ç›®å½•ï¼š

```
data/documents/
â”œâ”€â”€ AIåŸºç¡€çŸ¥è¯†.pdf
â”œâ”€â”€ æœºå™¨å­¦ä¹ æ•™ç¨‹.docx
â””â”€â”€ æ·±åº¦å­¦ä¹ ç¬”è®°.txt
```

æ”¯æŒçš„æ–‡æ¡£æ ¼å¼ï¼š
- PDF (.pdf)
- Wordæ–‡æ¡£ (.docx, .doc)
- æ–‡æœ¬æ–‡ä»¶ (.txt)

### ç¬¬ä¸‰æ­¥ï¼šå¼€å§‹ä½¿ç”¨

é€‰æ‹©æ‚¨å–œæ¬¢çš„æ–¹å¼ï¼š

#### æ–¹å¼1ï¼šWebç•Œé¢ï¼ˆæ¨èï¼‰
```bash
python run.py --mode web
```
ç„¶åè®¿é—® http://localhost:7860

#### æ–¹å¼2ï¼šå‘½ä»¤è¡Œ
```bash
python run.py --mode cli
```

## è¯¦ç»†é…ç½®

### ç¯å¢ƒå˜é‡é…ç½®

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼š

```bash
# ============ æ¨¡å‹é…ç½® ============

# ä½¿ç”¨æ¨¡å¼ï¼šlocalï¼ˆæœ¬åœ°ï¼‰æˆ– apiï¼ˆåœ¨çº¿APIï¼‰
MODEL_TYPE=local

# å¦‚æœä½¿ç”¨åœ¨çº¿APIï¼Œé…ç½®ä»¥ä¸‹å‚æ•°
OPENAI_API_KEY=sk-your-api-key
OPENAI_BASE_URL=https://api.openai.com/v1

# ============ åµŒå…¥æ¨¡å‹ ============

# ä¸­æ–‡åœºæ™¯æ¨è
EMBEDDING_MODEL=BAAI/bge-base-zh-v1.5
# æˆ–ä½¿ç”¨ï¼šBAAI/bge-large-zh-v1.5 (æ›´å¥½ä½†æ›´æ…¢)

# å¤šè¯­è¨€åœºæ™¯
# EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# ============ å‘é‡æ•°æ®åº“ ============

# ç±»å‹ï¼šfaiss æˆ– chroma
VECTOR_DB_TYPE=faiss

# å­˜å‚¨è·¯å¾„
VECTOR_DB_PATH=./data/vectordb

# ============ æ£€ç´¢é…ç½® ============

# Top-Kï¼šè¿”å›æœ€ç›¸å…³çš„Kä¸ªæ–‡æ¡£ç‰‡æ®µ
TOP_K=5

# æ–‡æ¡£åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
CHUNK_SIZE=500

# æ–‡æ¡£åˆ†å—é‡å ï¼ˆå­—ç¬¦æ•°ï¼‰
CHUNK_OVERLAP=50

# ============ æ—¥å¿—é…ç½® ============

# æ—¥å¿—çº§åˆ«ï¼šDEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO
```

### åµŒå…¥æ¨¡å‹é€‰æ‹©æŒ‡å—

| æ¨¡å‹ | è¯­è¨€ | ç»´åº¦ | æ€§èƒ½ | æ¨èåœºæ™¯ |
|------|------|------|------|---------|
| BAAI/bge-base-zh-v1.5 | ä¸­æ–‡ | 768 | å¿«é€Ÿ | ä¸­æ–‡æ–‡æ¡£ï¼Œå¹³è¡¡æ€§èƒ½ |
| BAAI/bge-large-zh-v1.5 | ä¸­æ–‡ | 1024 | è¾ƒæ…¢ | ä¸­æ–‡æ–‡æ¡£ï¼Œé«˜è´¨é‡ |
| paraphrase-multilingual-MiniLM-L12-v2 | å¤šè¯­è¨€ | 384 | æœ€å¿« | å¤šè¯­è¨€ï¼Œå¿«é€Ÿæ£€ç´¢ |
| all-MiniLM-L6-v2 | è‹±æ–‡ | 384 | æœ€å¿« | è‹±æ–‡æ–‡æ¡£ |

### æ–‡æ¡£åˆ†å—å‚æ•°è°ƒä¼˜

**CHUNK_SIZEï¼ˆåˆ†å—å¤§å°ï¼‰**
- è¾ƒå°ï¼ˆ200-300ï¼‰ï¼šæ£€ç´¢æ›´ç²¾ç¡®ï¼Œä½†å¯èƒ½ä¸¢å¤±ä¸Šä¸‹æ–‡
- ä¸­ç­‰ï¼ˆ500-800ï¼‰ï¼šå¹³è¡¡ç²¾ç¡®åº¦å’Œä¸Šä¸‹æ–‡ï¼ˆæ¨èï¼‰
- è¾ƒå¤§ï¼ˆ1000+ï¼‰ï¼šä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡ï¼Œä½†æ£€ç´¢å¯èƒ½ä¸å¤Ÿç²¾ç¡®

**CHUNK_OVERLAPï¼ˆé‡å å¤§å°ï¼‰**
- å»ºè®®è®¾ç½®ä¸º chunk_size çš„ 10-20%
- é¿å…åœ¨åˆ†å—è¾¹ç•Œä¸¢å¤±é‡è¦ä¿¡æ¯

## ä½¿ç”¨ç¤ºä¾‹

### 1. Webç•Œé¢ä½¿ç”¨

#### æ­¥éª¤1ï¼šåˆå§‹åŒ–ç³»ç»Ÿ
1. æ‰“å¼€Webç•Œé¢
2. è¿›å…¥"ç³»ç»Ÿåˆå§‹åŒ–"æ ‡ç­¾
3. é…ç½®å‚æ•°ï¼š
   - æ–‡æ¡£è·¯å¾„ï¼š`data/documents`
   - åµŒå…¥æ¨¡å‹ï¼š`BAAI/bge-base-zh-v1.5`
   - æ–‡æ¡£å—å¤§å°ï¼š`500`
   - æ–‡æ¡£å—é‡å ï¼š`50`
4. ç‚¹å‡»"åˆå§‹åŒ–ç³»ç»Ÿ"æŒ‰é’®
5. ç­‰å¾…åˆå§‹åŒ–å®Œæˆ

#### æ­¥éª¤2ï¼šé—®ç­”
1. è¿›å…¥"é—®ç­”"æ ‡ç­¾
2. è¾“å…¥é—®é¢˜ï¼Œä¾‹å¦‚ï¼š"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
3. é€‰æ‹©æ£€ç´¢å™¨ç±»å‹ï¼šFAISS / BM25 / Hybrid
4. è®¾ç½®Top-Kå€¼ï¼ˆå»ºè®®3-5ï¼‰
5. å‹¾é€‰"æ˜¾ç¤ºæ¥æºæ–‡æ¡£"
6. ç‚¹å‡»"æŸ¥è¯¢"æŒ‰é’®
7. æŸ¥çœ‹ç­”æ¡ˆå’Œå‚è€ƒæ¥æº

#### æ­¥éª¤3ï¼šå¯¹æ¯”æ£€ç´¢å™¨
1. è¿›å…¥"æ£€ç´¢å™¨å¯¹æ¯”"æ ‡ç­¾
2. è¾“å…¥ç›¸åŒçš„é—®é¢˜
3. ç‚¹å‡»"å¼€å§‹å¯¹æ¯”"
4. æŸ¥çœ‹ä¸åŒæ£€ç´¢å™¨çš„ç»“æœå·®å¼‚

### 2. CLIå‘½ä»¤è¡Œä½¿ç”¨

#### äº¤äº’æ¨¡å¼

```bash
# å¯åŠ¨äº¤äº’æ¨¡å¼
python cli_demo.py --interactive

# åœ¨äº¤äº’æ¨¡å¼ä¸­ï¼š
>>> ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ        # ç›´æ¥è¾“å…¥é—®é¢˜
>>> /retriever faiss        # åˆ‡æ¢æ£€ç´¢å™¨
>>> /topk 3                 # è®¾ç½®Top-K
>>> /compare æœºå™¨å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ  # å¯¹æ¯”æ£€ç´¢å™¨
>>> /help                   # æ˜¾ç¤ºå¸®åŠ©
>>> /quit                   # é€€å‡º
```

#### å•æ¬¡æŸ¥è¯¢æ¨¡å¼

```bash
# åŸºæœ¬æŸ¥è¯¢
python cli_demo.py --question "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"

# æŒ‡å®šæ£€ç´¢å™¨
python cli_demo.py \
    --question "æœºå™¨å­¦ä¹ çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ" \
    --retriever hybrid \
    --topk 5

# å¯¹æ¯”æ£€ç´¢å™¨
python cli_demo.py \
    --question "äººå·¥æ™ºèƒ½çš„æœªæ¥è¶‹åŠ¿" \
    --compare
```

### 3. ç¼–ç¨‹æ–¹å¼ä½¿ç”¨

```python
from src.document_loader import DocumentProcessor
from src.vector_store import VectorStoreManager
from src.retriever import RetrieverManager
from src.generator import AnswerGenerator, RAGPipeline
import config

# 1. åŠ è½½å’Œç´¢å¼•æ–‡æ¡£
processor = DocumentProcessor()
chunks = processor.process_documents("data/documents")

# 2. åˆ›å»ºå‘é‡ç´¢å¼•
vs_manager = VectorStoreManager()
vs_manager.create_vectorstore(chunks)
vs_manager.save("my_index")

# 3. åˆå§‹åŒ–æ£€ç´¢å™¨
retriever = RetrieverManager(vs_manager)
retriever.setup_bm25(chunks)

# 4. åˆ›å»ºRAGæµæ°´çº¿
generator = AnswerGenerator()
rag = RAGPipeline(retriever, generator)

# 5. æ‰§è¡ŒæŸ¥è¯¢
result = rag.query(
    question="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
    retriever_type="hybrid",
    k=5
)

# 6. è·å–ç»“æœ
print(f"ç­”æ¡ˆ: {result['answer']}")
print(f"æ¥æºæ•°é‡: {len(result['sources'])}")
```

## é«˜çº§åŠŸèƒ½

### 1. è‡ªå®šä¹‰æ£€ç´¢ç­–ç•¥

#### æ··åˆæ£€ç´¢æƒé‡è°ƒæ•´

```python
from src.retriever import RetrieverManager

retriever = RetrieverManager(vs_manager)

# è°ƒæ•´FAISSå’ŒBM25çš„æƒé‡
results = retriever.hybrid_retrieve(
    query="ä½ çš„é—®é¢˜",
    k=5,
    faiss_weight=0.7  # FAISSæƒé‡70%ï¼ŒBM25æƒé‡30%
)
```

### 2. æ‰¹é‡è¯„ä¼°

```python
# åˆ›å»ºæµ‹è¯•ç”¨ä¾‹æ–‡ä»¶ test_cases.json
[
    {
        "question": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "ground_truth": "äººå·¥æ™ºèƒ½æ˜¯...",
        "expected_keywords": ["AI", "è®¡ç®—æœº", "æ™ºèƒ½"]
    },
    {
        "question": "æœºå™¨å­¦ä¹ çš„åº”ç”¨ï¼Ÿ",
        "ground_truth": "æœºå™¨å­¦ä¹ åº”ç”¨åŒ…æ‹¬...",
        "expected_keywords": ["åº”ç”¨", "å›¾åƒ", "è¯­éŸ³"]
    }
]

# è¿è¡Œè¯„ä¼°
python evaluate_rag.py --test-file test_cases.json
```

### 3. è‡ªå®šä¹‰æç¤ºæ¨¡æ¿

```python
from langchain.prompts import PromptTemplate

# è‡ªå®šä¹‰æç¤ºæ¨¡æ¿
custom_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼Œè¦æ±‚ï¼š
1. ç­”æ¡ˆç®€æ´æ˜äº†
2. ç”¨ä¸­æ–‡å›ç­”
3. æ ‡æ³¨ä¿¡æ¯æ¥æº

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""
)

# ä½¿ç”¨è‡ªå®šä¹‰æ¨¡æ¿
from src.generator import AnswerGenerator
generator = AnswerGenerator()
generator.prompt_template = custom_template
```

### 4. å¢é‡ç´¢å¼•

```python
# åŠ è½½å·²æœ‰ç´¢å¼•
vs_manager = VectorStoreManager()
vs_manager.load("existing_index")

# å¤„ç†æ–°æ–‡æ¡£
processor = DocumentProcessor()
new_chunks = processor.process_documents("new_documents/")

# æ·»åŠ åˆ°ç°æœ‰ç´¢å¼•ï¼ˆFAISSï¼‰
from langchain_community.vectorstores import FAISS
new_db = FAISS.from_documents(new_chunks, vs_manager.embeddings)
vs_manager.vectorstore.merge_from(new_db)

# ä¿å­˜æ›´æ–°åçš„ç´¢å¼•
vs_manager.save("existing_index")
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æ¨¡å—å¯¼å…¥é”™è¯¯

**é—®é¢˜**: `ModuleNotFoundError: No module named 'langchain'`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç¡®ä¿åœ¨è™šæ‹Ÿç¯å¢ƒä¸­
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\Scripts\activate     # Windows

# é‡æ–°å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

#### 2. åµŒå…¥æ¨¡å‹ä¸‹è½½å¤±è´¥

**é—®é¢˜**: æ— æ³•ä¸‹è½½HuggingFaceæ¨¡å‹

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ³•1ï¼šä½¿ç”¨é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# æ–¹æ³•2ï¼šæ‰‹åŠ¨ä¸‹è½½æ¨¡å‹
# 1. è®¿é—® https://hf-mirror.com/BAAI/bge-base-zh-v1.5
# 2. ä¸‹è½½æ‰€æœ‰æ–‡ä»¶åˆ°æœ¬åœ°ç›®å½•
# 3. ä¿®æ”¹.envä¸­çš„EMBEDDING_MODELä¸ºæœ¬åœ°è·¯å¾„
EMBEDDING_MODEL=/path/to/local/model
```

#### 3. å†…å­˜ä¸è¶³

**é—®é¢˜**: `MemoryError` æˆ–ç³»ç»Ÿå¡é¡¿

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. å‡å°chunk_size
CHUNK_SIZE=300

# 2. å‡å°top_k
TOP_K=3

# 3. ä½¿ç”¨æ›´å°çš„åµŒå…¥æ¨¡å‹
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# 4. æ‰¹é‡å¤„ç†æ–‡æ¡£
# åˆ†æ‰¹ç´¢å¼•å¤§é‡æ–‡æ¡£
```

#### 4. FAISSç´¢å¼•é”™è¯¯

**é—®é¢˜**: `RuntimeError: Error in void faiss::...`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# åˆ é™¤ç°æœ‰ç´¢å¼•é‡æ–°åˆ›å»º
rm -rf data/vectordb/*

# æˆ–å¼ºåˆ¶é‡æ–°ç´¢å¼•
python cli_demo.py --force-reindex
```

#### 5. LLM APIé”™è¯¯

**é—®é¢˜**: `openai.error.AuthenticationError`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥.envé…ç½®
# 1. ç¡®ä¿APIå¯†é’¥æ­£ç¡®
OPENAI_API_KEY=sk-your-correct-key

# 2. æ£€æŸ¥APIåœ°å€
OPENAI_BASE_URL=https://api.openai.com/v1

# 3. å¦‚æœä½¿ç”¨ä»£ç†
export HTTP_PROXY=http://proxy:port
export HTTPS_PROXY=http://proxy:port
```

#### 6. å‘é‡æ£€ç´¢ç»“æœä¸å‡†ç¡®

**è§£å†³æ–¹æ¡ˆ**:

1. **ä¼˜åŒ–æ–‡æ¡£åˆ†å—**
```bash
# è°ƒæ•´åˆ†å—å‚æ•°
CHUNK_SIZE=800
CHUNK_OVERLAP=100
```

2. **æ›´æ¢åµŒå…¥æ¨¡å‹**
```bash
# ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹
EMBEDDING_MODEL=BAAI/bge-large-zh-v1.5
```

3. **ä½¿ç”¨æ··åˆæ£€ç´¢**
```python
# ç»“åˆå‘é‡æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢
result = rag.query(
    question="ä½ çš„é—®é¢˜",
    retriever_type="hybrid"
)
```

4. **å¢åŠ Top-K**
```bash
TOP_K=10  # æ£€ç´¢æ›´å¤šæ–‡æ¡£
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

#### 1. åŠ é€ŸåµŒå…¥ç¼–ç 

```python
# ä½¿ç”¨GPU
from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-base-zh-v1.5",
    model_kwargs={'device': 'cuda'},  # ä½¿ç”¨GPU
    encode_kwargs={'normalize_embeddings': True}
)
```

#### 2. æ‰¹é‡å¤„ç†

```python
# æ‰¹é‡ç¼–ç æ–‡æ¡£
batch_size = 32
for i in range(0, len(chunks), batch_size):
    batch = chunks[i:i+batch_size]
    # å¤„ç†æ‰¹æ¬¡
```

#### 3. ä½¿ç”¨æ›´å¿«çš„å‘é‡æ•°æ®åº“

```bash
# Chromaå¯èƒ½æ¯”FAISSæ›´å¿«ï¼ˆå–å†³äºåœºæ™¯ï¼‰
VECTOR_DB_TYPE=chroma
```

### æ—¥å¿—å’Œè°ƒè¯•

#### å¯ç”¨è°ƒè¯•æ—¥å¿—

```bash
# ä¿®æ”¹.env
LOG_LEVEL=DEBUG
```

#### æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶

```bash
# æ—¥å¿—æ–‡ä»¶ä½ç½®
data/logs/
â”œâ”€â”€ web_demo.log
â”œâ”€â”€ cli_demo.log
â”œâ”€â”€ evaluation.log
â””â”€â”€ run.log

# æŸ¥çœ‹æœ€æ–°æ—¥å¿—
tail -f data/logs/web_demo.log
```

## æ›´å¤šå¸®åŠ©

- ğŸ“– æŸ¥çœ‹ [README.md](../README.md) äº†è§£é¡¹ç›®æ¦‚è¿°
- ğŸ› é‡åˆ°é—®é¢˜ï¼Ÿæäº¤ [Issue](https://github.com/your-repo/issues)
- ğŸ’¬ åŠ å…¥è®¨è®ºï¼š[Discussions](https://github.com/your-repo/discussions)
- ğŸ“§ è”ç³»ä½œè€…ï¼šyour.email@example.com

---

**æç¤º**: æœ¬æŒ‡å—æŒç»­æ›´æ–°ä¸­ï¼Œæ¬¢è¿æå‡ºæ”¹è¿›å»ºè®®ï¼

