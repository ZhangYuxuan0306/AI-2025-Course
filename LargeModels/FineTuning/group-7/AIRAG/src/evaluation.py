"""è¯„ä¼°æ¨¡å—ï¼šä½¿ç”¨RAGASè¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°"""
from typing import List, Dict
import time
from datetime import datetime
import pandas as pd
from loguru import logger
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
    answer_similarity,
    answer_correctness
)
from datasets import Dataset
import config


class RAGEvaluator:
    """RAGç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self):
        self.metrics = [
            faithfulness,           # å¿ å®åº¦ï¼šç­”æ¡ˆæ˜¯å¦åŸºäºä¸Šä¸‹æ–‡
            answer_relevancy,       # ç­”æ¡ˆç›¸å…³æ€§
            context_precision,      # ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦
            context_recall,         # ä¸Šä¸‹æ–‡å¬å›ç‡
        ]
    
    def prepare_evaluation_dataset(
        self,
        questions: List[str],
        answers: List[str],
        contexts: List[List[str]],
        ground_truths: List[str] = None
    ) -> Dataset:
        """å‡†å¤‡è¯„ä¼°æ•°æ®é›†"""
        data = {
            'question': questions,
            'answer': answers,
            'contexts': contexts,
        }
        
        if ground_truths:
            data['ground_truth'] = ground_truths
        
        dataset = Dataset.from_dict(data)
        logger.info(f"å‡†å¤‡è¯„ä¼°æ•°æ®é›†ï¼Œå…± {len(questions)} ä¸ªæ ·æœ¬")
        return dataset
    
    def evaluate_rag_system(
        self,
        dataset: Dataset,
        metrics: List = None
    ) -> Dict:
        """è¯„ä¼°RAGç³»ç»Ÿ"""
        metrics = metrics or self.metrics
        
        try:
            logger.info("å¼€å§‹RAGASè¯„ä¼°...")
            start_time = time.time()
            
            result = evaluate(
                dataset=dataset,
                metrics=metrics,
            )
            
            elapsed_time = time.time() - start_time
            logger.info(f"è¯„ä¼°å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
            
            return {
                'scores': result,
                'elapsed_time': elapsed_time,
                'timestamp': datetime.now().isoformat()
            }
        
        except Exception as e:
            logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
            return {
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def calculate_retrieval_metrics(
        self,
        retrieved_docs: List[List[str]],
        relevant_docs: List[List[str]]
    ) -> Dict:
        """è®¡ç®—æ£€ç´¢æŒ‡æ ‡ï¼šå¬å›ç‡å’Œç²¾ç¡®åº¦"""
        metrics = {
            'precision': [],
            'recall': [],
            'f1': [],
            'mrr': [],  # Mean Reciprocal Rank
            'hit_rate': []
        }
        
        for retrieved, relevant in zip(retrieved_docs, relevant_docs):
            retrieved_set = set(retrieved)
            relevant_set = set(relevant)
            
            # è®¡ç®—äº¤é›†
            hits = retrieved_set & relevant_set
            
            # ç²¾ç¡®åº¦
            precision = len(hits) / len(retrieved_set) if retrieved_set else 0
            metrics['precision'].append(precision)
            
            # å¬å›ç‡
            recall = len(hits) / len(relevant_set) if relevant_set else 0
            metrics['recall'].append(recall)
            
            # F1åˆ†æ•°
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            metrics['f1'].append(f1)
            
            # Hit Rate
            hit_rate = 1 if hits else 0
            metrics['hit_rate'].append(hit_rate)
            
            # MRR
            mrr = 0
            for i, doc in enumerate(retrieved, 1):
                if doc in relevant_set:
                    mrr = 1 / i
                    break
            metrics['mrr'].append(mrr)
        
        # è®¡ç®—å¹³å‡å€¼
        avg_metrics = {
            f'avg_{key}': sum(values) / len(values) if values else 0
            for key, values in metrics.items()
        }
        
        logger.info(f"æ£€ç´¢æŒ‡æ ‡: {avg_metrics}")
        return avg_metrics
    
    def measure_performance(
        self,
        rag_pipeline,
        test_questions: List[str],
        retriever_types: List[str] = None
    ) -> Dict:
        """æµ‹é‡æ€§èƒ½æŒ‡æ ‡ï¼šå»¶è¿Ÿå’Œååé‡"""
        retriever_types = retriever_types or ['faiss', 'bm25', 'hybrid']
        
        performance_results = {}
        
        for retriever_type in retriever_types:
            latencies = []
            
            logger.info(f"æµ‹è¯• {retriever_type} æ£€ç´¢å™¨æ€§èƒ½...")
            
            for question in test_questions:
                start_time = time.time()
                try:
                    result = rag_pipeline.query(question, retriever_type=retriever_type)
                    latency = time.time() - start_time
                    latencies.append(latency)
                except Exception as e:
                    logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
                    continue
            
            if latencies:
                performance_results[retriever_type] = {
                    'avg_latency': sum(latencies) / len(latencies),
                    'min_latency': min(latencies),
                    'max_latency': max(latencies),
                    'throughput': len(latencies) / sum(latencies),  # queries per second
                    'num_queries': len(latencies)
                }
        
        logger.info(f"æ€§èƒ½æµ‹è¯•å®Œæˆ")
        return performance_results
    
    def generate_evaluation_report(
        self,
        ragas_results: Dict,
        retrieval_metrics: Dict,
        performance_metrics: Dict,
        output_path: str = None
    ) -> pd.DataFrame:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report_data = []
        
        # RAGASè¯„åˆ†
        if 'scores' in ragas_results:
            for metric, score in ragas_results['scores'].items():
                report_data.append({
                    'Category': 'RAGAS',
                    'Metric': metric,
                    'Value': score
                })
        
        # æ£€ç´¢æŒ‡æ ‡
        for metric, value in retrieval_metrics.items():
            report_data.append({
                'Category': 'Retrieval',
                'Metric': metric,
                'Value': value
            })
        
        # æ€§èƒ½æŒ‡æ ‡
        for retriever, metrics in performance_metrics.items():
            for metric, value in metrics.items():
                report_data.append({
                    'Category': f'Performance-{retriever}',
                    'Metric': metric,
                    'Value': value
                })
        
        df = pd.DataFrame(report_data)
        
        if output_path:
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        
        return df


class FailureAnalyzer:
    """å¤±è´¥æ¡ˆä¾‹åˆ†æå™¨ï¼šè¯¯å·®å½’å› """
    
    def __init__(self):
        self.failure_types = {
            'retrieval_error': 'æ£€ç´¢é”™è¯¯ï¼šæœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£',
            'ranking_error': 'æ’åºé”™è¯¯ï¼šç›¸å…³æ–‡æ¡£æ’åºé å',
            'generation_error': 'ç”Ÿæˆé”™è¯¯ï¼šç”Ÿæˆçš„ç­”æ¡ˆä¸å‡†ç¡®',
            'context_error': 'ä¸Šä¸‹æ–‡é”™è¯¯ï¼šæ–‡æ¡£åˆ†å—ä¸å½“',
            'low_confidence': 'ä½ç½®ä¿¡åº¦ï¼šæ£€ç´¢æ–‡æ¡£ç›¸å…³åº¦è¿‡ä½',
            'no_answer': 'æ— æ³•å›ç­”ï¼šæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯'
        }
    
    def analyze_realtime(self, question: str, answer: str, 
                        retrieved_docs: list, scores: list = None):
        """å®æ—¶åˆ†ææŸ¥è¯¢è´¨é‡ï¼ˆç”¨äºWebç•Œé¢ï¼‰"""
        analysis = {
            'quality_score': 0.0,  # 0-100åˆ†
            'issues': [],
            'warnings': [],
            'suggestions': [],
            'error_type': None,
            'severity': 'none'  # none, low, medium, high
        }
        
        # æ£€æŸ¥1ï¼šæ£€ç´¢æ–‡æ¡£æ•°é‡
        if not retrieved_docs or len(retrieved_docs) == 0:
            analysis['issues'].append({
                'type': 'retrieval_error',
                'message': 'âŒ æ£€ç´¢é”™è¯¯ï¼šæœªæ£€ç´¢åˆ°ä»»ä½•æ–‡æ¡£',
                'severity': 'high'
            })
            analysis['error_type'] = 'retrieval_error'
            analysis['severity'] = 'high'
            analysis['quality_score'] = 0
            analysis['suggestions'].append('å»ºè®®ï¼šæ£€æŸ¥æ–‡æ¡£åº“æ˜¯å¦åŒ…å«ç›¸å…³å†…å®¹ï¼Œæˆ–å°è¯•ä¸åŒçš„æ£€ç´¢å™¨')
            return analysis
        
        # æ£€æŸ¥2ï¼šæ£€ç´¢ç›¸å…³åº¦ï¼ˆä½¿ç”¨åˆ†æ•°ï¼‰
        if scores and len(scores) > 0:
            avg_score = sum(scores) / len(scores)
            max_score = max(scores)
            
            # ç›¸å…³åº¦è¿‡ä½
            if max_score < 0.3:
                analysis['issues'].append({
                    'type': 'low_confidence',
                    'message': f'âš ï¸ ä½ç½®ä¿¡åº¦ï¼šæœ€é«˜ç›¸å…³åº¦ä»… {max_score:.2f}ï¼ˆå»ºè®®>0.5ï¼‰',
                    'severity': 'high'
                })
                analysis['error_type'] = 'retrieval_error'
                analysis['severity'] = 'high'
                analysis['quality_score'] = max(0, max_score * 100)
                analysis['suggestions'].append('å»ºè®®ï¼šæ–‡æ¡£åº“å¯èƒ½ä¸åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œå°è¯•æ¢ç”¨ BM25 æˆ– Hybrid æ£€ç´¢å™¨')
            
            elif max_score < 0.5:
                analysis['warnings'].append({
                    'type': 'low_confidence',
                    'message': f'âš ï¸ ç›¸å…³åº¦ä¸€èˆ¬ï¼šæœ€é«˜ç›¸å…³åº¦ {max_score:.2f}',
                    'severity': 'medium'
                })
                analysis['severity'] = 'medium'
                analysis['quality_score'] = max_score * 100
                analysis['suggestions'].append('å»ºè®®ï¼šå¯ä»¥å°è¯•é‡æ–°è¡¨è¿°é—®é¢˜ï¼Œæˆ–ä½¿ç”¨ Hybrid æ£€ç´¢å™¨')
            
            elif max_score < 0.7:
                analysis['warnings'].append({
                    'type': 'medium_confidence',
                    'message': f'âœ“ ç›¸å…³åº¦å¯æ¥å—ï¼šæœ€é«˜ç›¸å…³åº¦ {max_score:.2f}',
                    'severity': 'low'
                })
                analysis['severity'] = 'low'
                analysis['quality_score'] = max_score * 100
            
            else:
                # ç›¸å…³åº¦è‰¯å¥½
                analysis['quality_score'] = max_score * 100
            
            # æ£€æŸ¥3ï¼šæ’åºé—®é¢˜ï¼ˆç›¸å…³æ–‡æ¡£æ˜¯å¦é å‰ï¼‰
            if len(scores) >= 3 and scores[0] < 0.5 and max_score > 0.7:
                # æœ€é«˜åˆ†åœ¨åé¢ï¼Œä½†ç¬¬ä¸€ä¸ªåˆ†æ•°ä½
                best_position = scores.index(max_score) + 1
                analysis['warnings'].append({
                    'type': 'ranking_error',
                    'message': f'âš ï¸ æ’åºé—®é¢˜ï¼šæœ€ç›¸å…³æ–‡æ¡£åœ¨ç¬¬ {best_position} ä½',
                    'severity': 'medium'
                })
                if analysis['severity'] == 'none':
                    analysis['severity'] = 'medium'
                analysis['suggestions'].append('å»ºè®®ï¼šå°è¯•ä½¿ç”¨ Hybrid æ£€ç´¢å™¨ä»¥æ”¹å–„æ’åº')
        
        # æ£€æŸ¥4ï¼šç­”æ¡ˆè´¨é‡ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰
        answer_lower = answer.lower()
        
        # æ£€æµ‹æ˜¯å¦æ˜¯æ‹’ç»å›ç­”çš„æ ‡å¿—
        refuse_keywords = [
            'æ— æ³•å›ç­”', 'ä¸èƒ½å›ç­”', 'æ²¡æœ‰ç›¸å…³ä¿¡æ¯', 'æ²¡æœ‰æ‰¾åˆ°',
            'æ ¹æ®æä¾›çš„ä¿¡æ¯æ— æ³•', 'æ–‡æ¡£ä¸­æ²¡æœ‰', 'æ— æ³•ç¡®å®š',
            'å¯¹ä¸èµ·', 'æŠ±æ­‰', 'sorry', 'cannot answer'
        ]
        
        if any(keyword in answer_lower for keyword in refuse_keywords):
            analysis['issues'].append({
                'type': 'no_answer',
                'message': 'âŒ æ— æ³•å›ç­”ï¼šLLMæ˜ç¡®è¡¨ç¤ºæ— æ³•å›ç­”æ­¤é—®é¢˜',
                'severity': 'high'
            })
            if analysis['error_type'] is None:
                analysis['error_type'] = 'generation_error'
            if analysis['severity'] in ['none', 'low']:
                analysis['severity'] = 'high'
            analysis['quality_score'] = min(analysis['quality_score'], 30)
            analysis['suggestions'].append('å»ºè®®ï¼šé—®é¢˜å¯èƒ½è¶…å‡ºæ–‡æ¡£åº“èŒƒå›´ï¼Œè¯·æ·»åŠ ç›¸å…³æ–‡æ¡£æˆ–é‡æ–°è¡¨è¿°é—®é¢˜')
        
        # æ£€æŸ¥5ï¼šç­”æ¡ˆé•¿åº¦ï¼ˆè¿‡çŸ­å¯èƒ½æ˜¯ç”Ÿæˆé—®é¢˜ï¼‰
        if len(answer) < 20:
            analysis['warnings'].append({
                'type': 'generation_error',
                'message': 'âš ï¸ ç­”æ¡ˆè¿‡çŸ­ï¼šå¯èƒ½ç”Ÿæˆè´¨é‡ä¸ä½³',
                'severity': 'medium'
            })
            if analysis['severity'] == 'none':
                analysis['severity'] = 'medium'
            analysis['suggestions'].append('å»ºè®®ï¼šæ£€æŸ¥LLMé…ç½®ï¼Œæˆ–å°è¯•æ›´è¯¦ç»†çš„é—®é¢˜')
        
        # æ£€æŸ¥6ï¼šæ˜¯å¦åŒ…å«å¼•ç”¨
        if '[æ¥æº' not in answer and 'æ¥æº' not in answer.lower():
            analysis['warnings'].append({
                'type': 'generation_error',
                'message': 'âš ï¸ ç¼ºå°‘å¼•ç”¨ï¼šç­”æ¡ˆæœªåŒ…å«æ¥æºæ ‡æ³¨',
                'severity': 'low'
            })
            if analysis['severity'] == 'none':
                analysis['severity'] = 'low'
        
        # è®¾ç½®é»˜è®¤è´¨é‡åˆ†æ•°ï¼ˆå¦‚æœè¿˜æ²¡è®¾ç½®ï¼‰
        if analysis['quality_score'] == 0 and not analysis['issues']:
            analysis['quality_score'] = 75  # é»˜è®¤è‰¯å¥½
        
        # æœ€ç»ˆè´¨é‡è¯„çº§
        if analysis['quality_score'] >= 80:
            analysis['quality_level'] = 'ä¼˜ç§€'
            analysis['quality_emoji'] = 'ğŸŸ¢'
        elif analysis['quality_score'] >= 60:
            analysis['quality_level'] = 'è‰¯å¥½'
            analysis['quality_emoji'] = 'ğŸŸ¡'
        elif analysis['quality_score'] >= 40:
            analysis['quality_level'] = 'ä¸€èˆ¬'
            analysis['quality_emoji'] = 'ğŸŸ '
        else:
            analysis['quality_level'] = 'è¾ƒå·®'
            analysis['quality_emoji'] = 'ğŸ”´'
        
        return analysis
    
    def analyze_failure(
        self,
        question: str,
        generated_answer: str,
        expected_answer: str,
        retrieved_docs: List[str],
        relevant_docs: List[str],
        scores: List[float] = None
    ) -> Dict:
        """åˆ†æå•ä¸ªå¤±è´¥æ¡ˆä¾‹"""
        analysis = {
            'question': question,
            'generated_answer': generated_answer,
            'expected_answer': expected_answer,
            'errors': []
        }
        
        # æ£€æŸ¥æ£€ç´¢é”™è¯¯
        retrieved_set = set(retrieved_docs)
        relevant_set = set(relevant_docs)
        
        if not (retrieved_set & relevant_set):
            analysis['errors'].append({
                'type': 'retrieval_error',
                'description': 'æœªæ£€ç´¢åˆ°ä»»ä½•ç›¸å…³æ–‡æ¡£',
                'severity': 'high'
            })
        
        # æ£€æŸ¥æ’åºé”™è¯¯
        if scores:
            relevant_positions = [
                i for i, doc in enumerate(retrieved_docs) 
                if doc in relevant_set
            ]
            
            if relevant_positions and min(relevant_positions) > 2:
                analysis['errors'].append({
                    'type': 'ranking_error',
                    'description': f'ç›¸å…³æ–‡æ¡£æ’åºé åï¼Œæœ€ä½³ä½ç½®: {min(relevant_positions) + 1}',
                    'severity': 'medium'
                })
        
        # æ£€æŸ¥ç”Ÿæˆé”™è¯¯ï¼ˆç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦ï¼‰
        if generated_answer and expected_answer:
            similarity = self._simple_similarity(generated_answer, expected_answer)
            if similarity < 0.5:
                analysis['errors'].append({
                    'type': 'generation_error',
                    'description': f'ç”Ÿæˆç­”æ¡ˆä¸æœŸæœ›ç­”æ¡ˆç›¸ä¼¼åº¦ä½: {similarity:.2f}',
                    'severity': 'high'
                })
        
        # æ£€æŸ¥ä¸Šä¸‹æ–‡é”™è¯¯
        if retrieved_docs:
            avg_doc_length = sum(len(doc) for doc in retrieved_docs) / len(retrieved_docs)
            if avg_doc_length < 50 or avg_doc_length > 2000:
                analysis['errors'].append({
                    'type': 'context_error',
                    'description': f'æ–‡æ¡£å—é•¿åº¦å¼‚å¸¸: {avg_doc_length:.0f}',
                    'severity': 'low'
                })
        
        return analysis
    
    def _simple_similarity(self, text1: str, text2: str) -> float:
        """ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—"""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        return len(intersection) / len(union)
    
    def batch_analyze_failures(
        self,
        test_cases: List[Dict]
    ) -> Dict:
        """æ‰¹é‡åˆ†æå¤±è´¥æ¡ˆä¾‹"""
        results = []
        error_stats = {error_type: 0 for error_type in self.failure_types.keys()}
        
        for case in test_cases:
            analysis = self.analyze_failure(
                question=case['question'],
                generated_answer=case['generated_answer'],
                expected_answer=case.get('expected_answer', ''),
                retrieved_docs=case['retrieved_docs'],
                relevant_docs=case.get('relevant_docs', []),
                scores=case.get('scores')
            )
            
            results.append(analysis)
            
            # ç»Ÿè®¡é”™è¯¯ç±»å‹
            for error in analysis['errors']:
                error_stats[error['type']] += 1
        
        logger.info(f"å¤±è´¥æ¡ˆä¾‹åˆ†æå®Œæˆï¼Œå…± {len(test_cases)} ä¸ªæ¡ˆä¾‹")
        logger.info(f"é”™è¯¯ç»Ÿè®¡: {error_stats}")
        
        return {
            'analyses': results,
            'error_statistics': error_stats,
            'total_cases': len(test_cases)
        }
    
    def generate_failure_report(
        self,
        failure_analysis: Dict,
        output_path: str = None
    ) -> pd.DataFrame:
        """ç”Ÿæˆå¤±è´¥æ¡ˆä¾‹åˆ†ææŠ¥å‘Š"""
        report_data = []
        
        for analysis in failure_analysis['analyses']:
            for error in analysis['errors']:
                report_data.append({
                    'Question': analysis['question'][:100],
                    'Error Type': error['type'],
                    'Description': error['description'],
                    'Severity': error['severity']
                })
        
        df = pd.DataFrame(report_data)
        
        if output_path:
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            logger.info(f"å¤±è´¥æ¡ˆä¾‹æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        
        return df


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    evaluator = RAGEvaluator()
    
    # ç¤ºä¾‹æ•°æ®
    questions = ["ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ", "æœºå™¨å­¦ä¹ çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ"]
    answers = ["äººå·¥æ™ºèƒ½æ˜¯...", "æœºå™¨å­¦ä¹ åº”ç”¨åŒ…æ‹¬..."]
    contexts = [
        ["äººå·¥æ™ºèƒ½å®šä¹‰æ–‡æ¡£1", "äººå·¥æ™ºèƒ½å®šä¹‰æ–‡æ¡£2"],
        ["æœºå™¨å­¦ä¹ åº”ç”¨æ–‡æ¡£1", "æœºå™¨å­¦ä¹ åº”ç”¨æ–‡æ¡£2"]
    ]
    
    dataset = evaluator.prepare_evaluation_dataset(questions, answers, contexts)
    print(f"æ•°æ®é›†å‡†å¤‡å®Œæˆ: {len(dataset)} ä¸ªæ ·æœ¬")

