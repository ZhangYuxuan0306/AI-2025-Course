"""å¢å¼ºç‰ˆ Web Demoï¼šæ›´ç¾è§‚çš„ Gradio ç•Œé¢ï¼ŒåŒ…å«å®Œæ•´è¯„ä¼°åŠŸèƒ½"""
import os
import gradio as gr
from pathlib import Path
from loguru import logger
import json
import time
import pandas as pd
import config

# é…ç½® HuggingFace é•œåƒï¼ˆè§£å†³æ¨¡å‹ä¸‹è½½é—®é¢˜ï¼‰
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
from src.document_loader import DocumentProcessor
from src.vector_store import VectorStoreManager
from src.retriever import RetrieverManager
from src.generator import AnswerGenerator, RAGPipeline
from src.evaluation import RAGEvaluator, FailureAnalyzer

# é…ç½®æ—¥å¿—
logger.add(
    config.LOGS_DIR / "web_demo.log",
    rotation="500 MB",
    level=config.LOG_LEVEL
)


class RAGWebDemo:
    """RAG Webæ¼”ç¤ºåº”ç”¨ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    
    def __init__(self):
        self.rag_pipeline = None
        self.retriever_manager = None
        self.vs_manager = None
        self.chunks = []
        self.is_initialized = False
        self.evaluator = RAGEvaluator()
        self.failure_analyzer = FailureAnalyzer()
        self.query_history = []
    
    def initialize_system(
        self,
        documents_path: str,
        embedding_model: str,
        chunk_size: int,
        chunk_overlap: int,
        progress=gr.Progress()
    ):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
        try:
            progress(0, desc="å¼€å§‹åˆå§‹åŒ–...")
            logger.info("å¼€å§‹åˆå§‹åŒ–RAGç³»ç»Ÿ...")
            
            # 1. åŠ è½½å’Œåˆ†å—æ–‡æ¡£
            progress(0.2, desc="åŠ è½½æ–‡æ¡£...")
            processor = DocumentProcessor(
                chunk_size=int(chunk_size),
                chunk_overlap=int(chunk_overlap)
            )
            self.chunks = processor.process_documents(documents_path)
            
            if not self.chunks:
                return "âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥æ–‡æ¡£è·¯å¾„", ""
            
            progress(0.4, desc="åˆ›å»ºå‘é‡ç´¢å¼•...")
            # 2. åˆ›å»ºå‘é‡æ•°æ®åº“
            self.vs_manager = VectorStoreManager(embedding_model_name=embedding_model)
            self.vs_manager.create_vectorstore(self.chunks)
            self.vs_manager.save("web_demo")
            
            progress(0.6, desc="åˆå§‹åŒ–æ£€ç´¢å™¨...")
            # 3. åˆå§‹åŒ–æ£€ç´¢å™¨
            self.retriever_manager = RetrieverManager(self.vs_manager)
            self.retriever_manager.setup_bm25(self.chunks)
            
            progress(0.8, desc="åˆå§‹åŒ–ç”Ÿæˆå™¨...")
            # 4. åˆå§‹åŒ–ç”Ÿæˆå™¨
            generator = AnswerGenerator()
            
            # 5. åˆ›å»ºRAGæµæ°´çº¿
            self.rag_pipeline = RAGPipeline(self.retriever_manager, generator)
            
            self.is_initialized = True
            progress(1.0, desc="å®Œæˆï¼")
            logger.info("RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            
            # ç”Ÿæˆåˆå§‹åŒ–æŠ¥å‘Š
            report = f"""## âœ… ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼

### ğŸ“Š ç»Ÿè®¡ä¿¡æ¯
- **æ–‡æ¡£å—æ•°**: {len(self.chunks)}
- **åµŒå…¥æ¨¡å‹**: {embedding_model}
- **åˆ†å—å¤§å°**: {chunk_size}
- **åˆ†å—é‡å **: {chunk_overlap}
- **å‘é‡æ•°æ®åº“**: FAISS

### ğŸ“ æ–‡æ¡£æ¥æº
"""
            # ç»Ÿè®¡æ–‡æ¡£æ¥æº
            sources = {}
            for chunk in self.chunks[:100]:  # åªç»Ÿè®¡å‰100ä¸ª
                source = chunk.metadata.get('source', 'æœªçŸ¥')
                sources[source] = sources.get(source, 0) + 1
            
            for source, count in list(sources.items())[:10]:
                report += f"- `{Path(source).name}`: {count} å—\n"
            
            if len(sources) > 10:
                report += f"- ... è¿˜æœ‰ {len(sources) - 10} ä¸ªæ–‡æ¡£\n"
            
            return report, "âœ… ç³»ç»Ÿå·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹é—®ç­”"
        
        except Exception as e:
            logger.error(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            return f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}", "âŒ åˆå§‹åŒ–å¤±è´¥"
    
    def query(
        ......
        self,
        question: str,
        retriever_type: str,
        top_k: int,
        show_sources: bool,
        progress=gr.Progress()
    ):
        """æ‰§è¡ŒæŸ¥è¯¢"""
        if not self.is_initialized:
            return "âŒ è¯·å…ˆåˆå§‹åŒ–ç³»ç»Ÿ", "", "", None, ""
        
        if not question.strip():
            return "âŒ è¯·è¾“å…¥é—®é¢˜", "", "", None, ""
        
        try:
            progress(0, desc="æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
            logger.info(f"æ”¶åˆ°æŸ¥è¯¢: {question}")
            
            start_time = time.time()
            
            # æ‰§è¡ŒRAGæŸ¥è¯¢
            result = self.rag_pipeline.query(
                question=question,
                retriever_type=retriever_type.lower(),
                k=int(top_k)
            )
            
            query_time = time.time() - start_time
            
            progress(0.5, desc="ç”Ÿæˆç­”æ¡ˆ...")
            
            # ========== æ–°å¢ï¼šå¤±è´¥æ¡ˆä¾‹åˆ†æ ==========
            progress(0.6, desc="åˆ†æç­”æ¡ˆè´¨é‡...")
            
            # æå–æ£€ç´¢åˆ†æ•°
            scores = []
            retrieved_docs = []
            if result.get('sources'):
                for source in result['sources']:
                    if source.get('score') != 'N/A':
                        scores.append(source['score'])
                    retrieved_docs.append(source['content'])
            
            # æ‰§è¡Œå®æ—¶è´¨é‡åˆ†æ
            quality_analysis = self.failure_analyzer.analyze_realtime(
                question=question,
                answer=result['answer'],
                retrieved_docs=retrieved_docs,
                scores=scores
            )
            
            # ä¿å­˜æŸ¥è¯¢å†å²ï¼ˆåŒ…å«è´¨é‡åˆ†æï¼‰
            self.query_history.append({
                'question': question,
                'retriever': retriever_type,
                'time': query_time,
                'answer': result['answer'],
                'quality_score': quality_analysis['quality_score'],
                'severity': quality_analysis['severity']
            })
            
            # ========== æ ¼å¼åŒ–è´¨é‡åˆ†æç»“æœ ==========
            quality_text = f"""# {quality_analysis['quality_emoji']} ç­”æ¡ˆè´¨é‡åˆ†æ

## ğŸ“Š è´¨é‡è¯„åˆ†
**æ€»åˆ†**: {quality_analysis['quality_score']:.1f}/100 | **ç­‰çº§**: {quality_analysis['quality_level']} | **ä¸¥é‡ç¨‹åº¦**: {quality_analysis['severity'].upper()}

"""
            
            # æ˜¾ç¤ºé—®é¢˜
            if quality_analysis['issues']:
                quality_text += "## âŒ å‘ç°çš„é—®é¢˜\n\n"
                for issue in quality_analysis['issues']:
                    quality_text += f"- **{issue['message']}**\n"
                    quality_text += f"  - ç±»å‹: `{issue['type']}`\n"
                    quality_text += f"  - ä¸¥é‡ç¨‹åº¦: `{issue['severity']}`\n\n"
            
            # æ˜¾ç¤ºè­¦å‘Š
            if quality_analysis['warnings']:
                quality_text += "## âš ï¸ è­¦å‘Šä¿¡æ¯\n\n"
                for warning in quality_analysis['warnings']:
                    quality_text += f"- {warning['message']}\n"
            
            # æ˜¾ç¤ºå»ºè®®
            if quality_analysis['suggestions']:
                quality_text += "\n## ğŸ’¡ æ”¹è¿›å»ºè®®\n\n"
                for suggestion in quality_analysis['suggestions']:
                    quality_text += f"- {suggestion}\n"
            
            # å¦‚æœæ²¡æœ‰é—®é¢˜
            if not quality_analysis['issues'] and not quality_analysis['warnings']:
                quality_text += "## âœ… è´¨é‡è‰¯å¥½\n\n"
                quality_text += "æœªå‘ç°æ˜æ˜¾é—®é¢˜ï¼Œç­”æ¡ˆè´¨é‡è‰¯å¥½ã€‚\n"
            
            # è¯¯å·®å½’å› 
            if quality_analysis.get('error_type'):
                quality_text += f"\n## ğŸ” è¯¯å·®å½’å› \n\n"
                quality_text += f"**ä¸»è¦é”™è¯¯ç±»å‹**: `{quality_analysis['error_type']}`\n\n"
                
                error_descriptions = {
                    'retrieval_error': '**æ£€ç´¢é”™è¯¯**ï¼šç³»ç»Ÿæœªèƒ½æ£€ç´¢åˆ°ä¸é—®é¢˜ç›¸å…³çš„æ–‡æ¡£ã€‚å¯èƒ½åŸå› ï¼š\n- æ–‡æ¡£åº“ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹\n- é—®é¢˜è¡¨è¿°ä¸æ–‡æ¡£ç”¨è¯å·®å¼‚è¾ƒå¤§\n- åµŒå…¥æ¨¡å‹æœªèƒ½ç†è§£é—®é¢˜è¯­ä¹‰',
                    'ranking_error': '**æ’åºé”™è¯¯**ï¼šç›¸å…³æ–‡æ¡£è¢«æ£€ç´¢åˆ°ä½†æ’åºé åã€‚å¯èƒ½åŸå› ï¼š\n- æ£€ç´¢å™¨è¯„åˆ†æœºåˆ¶ä¸å¤Ÿå‡†ç¡®\n- BM25å¯¹è¯­ä¹‰ç†è§£è¾ƒå¼±\n- éœ€è¦ä½¿ç”¨Hybridæ··åˆæ£€ç´¢',
                    'generation_error': '**ç”Ÿæˆé”™è¯¯**ï¼šLLMç”Ÿæˆçš„ç­”æ¡ˆè´¨é‡ä¸ä½³ã€‚å¯èƒ½åŸå› ï¼š\n- æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ç›¸å…³\n- LLMæœªèƒ½æ­£ç¡®ç†è§£ä¸Šä¸‹æ–‡\n- æç¤ºè¯è®¾è®¡éœ€è¦ä¼˜åŒ–',
                    'low_confidence': '**ä½ç½®ä¿¡åº¦**ï¼šæ£€ç´¢æ–‡æ¡£ç›¸å…³åº¦è¿‡ä½ã€‚å¯èƒ½åŸå› ï¼š\n- é—®é¢˜è¶…å‡ºæ–‡æ¡£åº“èŒƒå›´\n- éœ€è¦æ›´æ¢æ£€ç´¢ç­–ç•¥\n- è€ƒè™‘æ·»åŠ æ›´å¤šç›¸å…³æ–‡æ¡£'
                }
                
                if quality_analysis['error_type'] in error_descriptions:
                    quality_text += error_descriptions[quality_analysis['error_type']]
            
            # ========== æ ¼å¼åŒ–ç­”æ¡ˆ ==========
            answer = f"""## ğŸ’¬ å›ç­”

{result['answer']}

---
â±ï¸ **æŸ¥è¯¢è€—æ—¶**: {query_time:.3f}ç§’ | ğŸ” **æ£€ç´¢å™¨**: {retriever_type.upper()} | ğŸ“„ **Top-K**: {top_k} | {quality_analysis['quality_emoji']} **è´¨é‡**: {quality_analysis['quality_score']:.0f}/100
"""
            
            # æ ¼å¼åŒ–æ¥æº
            sources_text = ""
            sources_json = ""
            sources_df = None
            
            progress(0.8, desc="æ•´ç†æ¥æºä¿¡æ¯...")
            
            if show_sources and result.get('sources'):
                sources_text = "## ğŸ“š å‚è€ƒæ¥æº\n\n"
                
                sources_data = []
                for source in result['sources']:
                    idx = source['index']
                    content = source['content']
                    metadata = source.get('metadata', {})
                    score = source.get('score', 'N/A')
                    
                    # Markdownæ ¼å¼
                    sources_text += f"""### ğŸ“„ æ¥æº {idx}

**å†…å®¹é¢„è§ˆ**:  
{content[:300]}...

**æ–‡æ¡£**: `{Path(metadata.get('source', 'æœªçŸ¥')).name}`  
"""
                    if score != 'N/A':
                        sources_text += f"**ç›¸å…³åº¦**: `{score:.4f}` {'ğŸŸ¢' if score > 0.7 else 'ğŸŸ¡' if score > 0.5 else 'ğŸ”´'}\n"
                    
                    sources_text += "\n---\n\n"
                    
                    # DataFrameæ•°æ®
                    sources_data.append({
                        'åºå·': idx,
                        'å†…å®¹é¢„è§ˆ': content[:100] + '...',
                        'æ¥æºæ–‡ä»¶': Path(metadata.get('source', 'æœªçŸ¥')).name,
                        'ç›¸å…³åº¦': f"{score:.4f}" if score != 'N/A' else 'N/A'
                    })
                
                # åˆ›å»ºDataFrame
                sources_df = pd.DataFrame(sources_data)
                
                # JSONæ ¼å¼çš„æ¥æºä¿¡æ¯
                sources_json = json.dumps(
                    result['sources'],
                    ensure_ascii=False,
                    indent=2
                )
            
            progress(1.0, desc="å®Œæˆï¼")
            
            return answer, sources_text, sources_json, sources_df, quality_text
        
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
            return f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}", "", "", None, ""
    
    def compare_retrievers(self, question: str, top_k: int, progress=gr.Progress()):
        """å¯¹æ¯”ä¸åŒæ£€ç´¢å™¨"""
        if not self.is_initialized:
            return "âŒ è¯·å…ˆåˆå§‹åŒ–ç³»ç»Ÿ", None
        
        if not question.strip():
            return "âŒ è¯·è¾“å…¥é—®é¢˜", None
        
        try:
            progress(0, desc="å‡†å¤‡å¯¹æ¯”...")
            comparison = self.retriever_manager.compare_retrievers(
                question, 
                k=int(top_k)
            )
            
            # æ ¼å¼åŒ–å¯¹æ¯”ç»“æœ
            result_text = f"""# ğŸ” æ£€ç´¢å™¨æ€§èƒ½å¯¹æ¯”

**æŸ¥è¯¢é—®é¢˜**: {question}

---
"""
            
            comparison_data = []
            
            for i, retriever_name in enumerate(['faiss', 'bm25', 'hybrid'], 1):
                progress(i/4, desc=f"åˆ†æ {retriever_name.upper()}...")
                
                result_text += f"\n## {'ğŸš€' if retriever_name == 'hybrid' else 'ğŸ“Š'} {retriever_name.upper()} æ£€ç´¢å™¨\n\n"
                
                results = comparison[retriever_name]
                for j, doc_info in enumerate(results, 1):
                    result_text += f"### Top-{j}\n"
                    result_text += f"**å†…å®¹**: {doc_info['content'][:200]}...\n"
                    
                    if 'score' in doc_info:
                        score = doc_info['score']
                        result_text += f"**åˆ†æ•°**: `{score:.4f}` {'ğŸŸ¢' if score > 0.7 else 'ğŸŸ¡' if score > 0.5 else 'ğŸ”´'}\n"
                        
                        # æ·»åŠ åˆ°å¯¹æ¯”æ•°æ®
                        comparison_data.append({
                            'æ£€ç´¢å™¨': retriever_name.upper(),
                            'æ’å': j,
                            'å†…å®¹é¢„è§ˆ': doc_info['content'][:80] + '...',
                            'ç›¸å…³åº¦': f"{score:.4f}"
                        })
                    else:
                        comparison_data.append({
                            'æ£€ç´¢å™¨': retriever_name.upper(),
                            'æ’å': j,
                            'å†…å®¹é¢„è§ˆ': doc_info['content'][:80] + '...',
                            'ç›¸å…³åº¦': 'N/A'
                        })
                    
                    result_text += "\n"
                
                result_text += "---\n"
            
            progress(1.0, desc="å®Œæˆï¼")
            
            # åˆ›å»ºå¯¹æ¯”DataFrame
            comparison_df = pd.DataFrame(comparison_data)
            
            return result_text, comparison_df
        
        except Exception as e:
            logger.error(f"æ£€ç´¢å™¨å¯¹æ¯”å¤±è´¥: {e}")
            return f"âŒ å¯¹æ¯”å¤±è´¥: {str(e)}", None
    
    def run_performance_test(self, num_queries: int, progress=gr.Progress()):
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        if not self.is_initialized:
            return "âŒ è¯·å…ˆåˆå§‹åŒ–ç³»ç»Ÿ", None
        
        try:
            # ç”Ÿæˆæµ‹è¯•é—®é¢˜
            test_questions = [
                "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
                "æœºå™¨å­¦ä¹ çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ",
                "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„åŒºåˆ«ï¼Ÿ",
                "ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ",
                "è‡ªç„¶è¯­è¨€å¤„ç†æœ‰å“ªäº›åº”ç”¨ï¼Ÿ"
            ] * (int(num_queries) // 5 + 1)
            test_questions = test_questions[:int(num_queries)]
            
            progress(0.1, desc="å¼€å§‹æ€§èƒ½æµ‹è¯•...")
            
            # æµ‹é‡æ€§èƒ½
            performance = self.evaluator.measure_performance(
                self.rag_pipeline,
                test_questions,
                retriever_types=['faiss', 'bm25', 'hybrid']
            )
            
            progress(0.8, desc="ç”ŸæˆæŠ¥å‘Š...")
            
            # æ ¼å¼åŒ–ç»“æœ
            report = f"""# ğŸ“Š æ€§èƒ½æµ‹è¯•æŠ¥å‘Š

**æµ‹è¯•æŸ¥è¯¢æ•°**: {num_queries}

---

"""
            
            perf_data = []
            for retriever, metrics in performance.items():
                report += f"""## {retriever.upper()} æ£€ç´¢å™¨

- **å¹³å‡å»¶è¿Ÿ**: `{metrics['avg_latency']:.3f}` ç§’
- **æœ€å°å»¶è¿Ÿ**: `{metrics['min_latency']:.3f}` ç§’
- **æœ€å¤§å»¶è¿Ÿ**: `{metrics['max_latency']:.3f}` ç§’
- **ååé‡**: `{metrics['throughput']:.2f}` queries/sec
- **æ€»æŸ¥è¯¢æ•°**: {metrics['num_queries']}

"""
                perf_data.append({
                    'æ£€ç´¢å™¨': retriever.upper(),
                    'å¹³å‡å»¶è¿Ÿ(ç§’)': f"{metrics['avg_latency']:.3f}",
                    'æœ€å°å»¶è¿Ÿ(ç§’)': f"{metrics['min_latency']:.3f}",
                    'æœ€å¤§å»¶è¿Ÿ(ç§’)': f"{metrics['max_latency']:.3f}",
                    'ååé‡(q/s)': f"{metrics['throughput']:.2f}"
                })
            
            progress(1.0, desc="å®Œæˆï¼")
            
            perf_df = pd.DataFrame(perf_data)
            
            return report, perf_df
        
        except Exception as e:
            logger.error(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            return f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {str(e)}", None
    
    def get_query_history(self):
        """è·å–æŸ¥è¯¢å†å²"""
        if not self.query_history:
            return None
        
        return pd.DataFrame(self.query_history)


def create_enhanced_web_interface():
    """åˆ›å»ºå¢å¼ºç‰ˆ Gradio Web ç•Œé¢"""
    demo_app = RAGWebDemo()
    
    # è‡ªå®šä¹‰CSS
    custom_css = """
    .gradio-container {
        font-family: 'Arial', sans-serif;
    }
    .status-box {
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
    }
    .success { background-color: #d4edda; border: 1px solid #c3e6cb; }
    .error { background-color: #f8d7da; border: 1px solid #f5c6cb; }
    .info { background-color: #d1ecf1; border: 1px solid #bee5eb; }
    """
    
    with gr.Blocks(
        title="ğŸ¤– å¢å¼ºç‰ˆ RAG é—®ç­”ç³»ç»Ÿ",
        theme=gr.themes.Soft(
            primary_hue="blue",
            secondary_hue="cyan",
        ),
        css=custom_css
    ) as demo:
        
        gr.Markdown("""
        # ğŸ¤– RAG é—®ç­”ç³»ç»Ÿ - å¢å¼ºç‰ˆ
        
        åŸºäº **LangChain** çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ™ºèƒ½é—®ç­”ç³»ç»Ÿ
        
        [![GitHub](https://img.shields.io/badge/GitHub-é¡¹ç›®åœ°å€-blue)](https://github.com/your-repo)
        [![Python](https://img.shields.io/badge/Python-3.8+-green)](https://www.python.org/)
        [![LangChain](https://img.shields.io/badge/LangChain-0.3.0-orange)](https://python.langchain.com/)
        """)
        
        # çŠ¶æ€æ 
        with gr.Row():
            system_status = gr.Textbox(
                label="ç³»ç»ŸçŠ¶æ€",
                value="âš ï¸ æœªåˆå§‹åŒ–",
                interactive=False,
                scale=1
            )
        
        # Tab 1: ç³»ç»Ÿåˆå§‹åŒ–
        with gr.Tab("ğŸ“– ç³»ç»Ÿåˆå§‹åŒ–"):
            gr.Markdown("""
            ## ğŸš€ é…ç½®å¹¶åˆå§‹åŒ– RAG ç³»ç»Ÿ
            
            åœ¨å¼€å§‹ä½¿ç”¨å‰ï¼Œè¯·å…ˆé…ç½®ç³»ç»Ÿå‚æ•°å¹¶åˆå§‹åŒ–ã€‚åˆå§‹åŒ–è¿‡ç¨‹åŒ…æ‹¬ï¼š
            1. åŠ è½½å¹¶åˆ†å—æ–‡æ¡£
            2. åˆ›å»ºå‘é‡ç´¢å¼•
            3. åˆå§‹åŒ–æ£€ç´¢å™¨ï¼ˆFAISSã€BM25ã€æ··åˆï¼‰
            4. åˆå§‹åŒ–ç­”æ¡ˆç”Ÿæˆå™¨
            """)
            
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### âš™ï¸ é…ç½®å‚æ•°")
                    
                    docs_path_input = gr.Textbox(
                        label="ğŸ“ æ–‡æ¡£è·¯å¾„",
                        value=str(config.DOCUMENTS_PATH),
                        placeholder="è¾“å…¥æ–‡æ¡£ç›®å½•è·¯å¾„",
                        info="æ”¯æŒ PDFã€TXTã€DOCX ç­‰æ ¼å¼"
                    )
                    
                    embedding_model_input = gr.Textbox(
                        label="ğŸ¤– åµŒå…¥æ¨¡å‹",
                        value=config.EMBEDDING_MODEL,
                        placeholder="HuggingFace æ¨¡å‹åç§°",
                        info="æ¨è: BAAI/bge-base-zh-v1.5 æˆ– BAAI/bge-large-zh-v1.5"
                    )
                    
                    with gr.Row():
                        chunk_size_input = gr.Slider(
                            label="ğŸ“ æ–‡æ¡£å—å¤§å°",
                            minimum=100,
                            maximum=2000,
                            value=config.CHUNK_SIZE,
                            step=100,
                            info="æ¯ä¸ªæ–‡æ¡£å—çš„å­—ç¬¦æ•°"
                        )
                        
                        chunk_overlap_input = gr.Slider(
                            label="ğŸ”— æ–‡æ¡£å—é‡å ",
                            minimum=0,
                            maximum=200,
                            value=config.CHUNK_OVERLAP,
                            step=10,
                            info="ç›¸é‚»å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°"
                        )
                    
                    init_button = gr.Button(
                        "ğŸš€ åˆå§‹åŒ–ç³»ç»Ÿ",
                        variant="primary",
                        size="lg"
                    )
                
                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ“Š åˆå§‹åŒ–ç»“æœ")
                    init_output = gr.Markdown(
                        value="ç­‰å¾…åˆå§‹åŒ–..."
                    )
            
            init_button.click(
                fn=demo_app.initialize_system,
                inputs=[
                    docs_path_input,
                    embedding_model_input,
                    chunk_size_input,
                    chunk_overlap_input
                ],
                outputs=[init_output, system_status]
            )
        
        # Tab 2: æ™ºèƒ½é—®ç­”
        with gr.Tab("ğŸ’¬ æ™ºèƒ½é—®ç­”"):
            gr.Markdown("""
            ## ğŸ¯ æå‡ºé—®é¢˜ï¼Œè·å–åŸºäºæ–‡æ¡£çš„ç­”æ¡ˆ
            
            ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œå¹¶ç”Ÿæˆå¸¦å¼•ç”¨çš„å‡†ç¡®ç­”æ¡ˆã€‚
            """)
            
            with gr.Row():
                with gr.Column(scale=2):
                    question_input = gr.Textbox(
                        label="â“ è¾“å…¥æ‚¨çš„é—®é¢˜",
                        placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿæœºå™¨å­¦ä¹ æœ‰å“ªäº›åº”ç”¨ï¼Ÿ",
                        lines=3
                    )
                    
                    with gr.Row():
                        retriever_type = gr.Radio(
                            label="ğŸ” æ£€ç´¢å™¨ç±»å‹",
                            choices=["FAISS", "BM25", "Hybrid"],
                            value="Hybrid",
                            info="Hybrid æ··åˆæ£€ç´¢é€šå¸¸æ•ˆæœæœ€å¥½"
                        )
                        
                        top_k_slider = gr.Slider(
                            label="ğŸ“Š Top-K",
                            minimum=1,
                            maximum=10,
                            value=config.TOP_K,
                            step=1,
                            info="è¿”å›æœ€ç›¸å…³çš„ K ä¸ªæ–‡æ¡£"
                        )
                    
                    show_sources_checkbox = gr.Checkbox(
                        label="ğŸ“š æ˜¾ç¤ºæ¥æºæ–‡æ¡£",
                        value=True
                    )
                    
                    query_button = gr.Button(
                        "ğŸ” å¼€å§‹æŸ¥è¯¢",
                        variant="primary",
                        size="lg"
                    )
                
                with gr.Column(scale=3):
                    answer_output = gr.Markdown(
                        label="ğŸ’¡ ç­”æ¡ˆ"
                    )
            
            # ========== æ–°å¢ï¼šè´¨é‡åˆ†ææ˜¾ç¤ºåŒºåŸŸ ==========
            with gr.Accordion("ğŸ” ç­”æ¡ˆè´¨é‡åˆ†æä¸å¤±è´¥æ¡ˆä¾‹å½’å› ", open=True):
                quality_analysis_output = gr.Markdown(
                    value="ç­‰å¾…æŸ¥è¯¢åæ˜¾ç¤ºè´¨é‡åˆ†æ..."
                )
            
            with gr.Accordion("ğŸ“š å‚è€ƒæ¥æºè¯¦æƒ…", open=False):
                with gr.Tabs():
                    with gr.Tab("ğŸ“ æ¥æºæ–‡æœ¬"):
                        sources_output = gr.Markdown()
                    
                    with gr.Tab("ğŸ“Š æ¥æºè¡¨æ ¼"):
                        sources_table = gr.Dataframe(
                            headers=["åºå·", "å†…å®¹é¢„è§ˆ", "æ¥æºæ–‡ä»¶", "ç›¸å…³åº¦"],
                            interactive=False
                        )
                    
                    with gr.Tab("ğŸ”§ JSON æ•°æ®"):
                        sources_json_output = gr.Code(
                            label="æ¥æº JSON",
                            language="json"
                        )
            
            query_button.click(
                fn=demo_app.query,
                inputs=[
                    question_input,
                    retriever_type,
                    top_k_slider,
                    show_sources_checkbox
                ],
                outputs=[
                    answer_output,
                    sources_output,
                    sources_json_output,
                    sources_table,
                    quality_analysis_output  # æ–°å¢è¾“å‡º
                ]
            )
            
            # ç¤ºä¾‹é—®é¢˜
            gr.Markdown("### ğŸ’¡ è¯•è¯•è¿™äº›ç¤ºä¾‹é—®é¢˜")
            with gr.Row():
                gr.Markdown("""
                **é«˜è´¨é‡é—®é¢˜ç¤ºä¾‹**ï¼ˆæ–‡æ¡£ä¸­æœ‰ç›¸å…³å†…å®¹ï¼‰ï¼š
                - ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ
                - æœºå™¨å­¦ä¹ çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ
                
                **ä½è´¨é‡é—®é¢˜ç¤ºä¾‹**ï¼ˆæµ‹è¯•å¤±è´¥æ¡ˆä¾‹åˆ†æï¼‰ï¼š
                - å¦‚ä½•åšçº¢çƒ§è‚‰ï¼Ÿï¼ˆä¸æ–‡æ¡£æ— å…³ï¼‰
                - æ˜å¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿï¼ˆæ–‡æ¡£èŒƒå›´å¤–ï¼‰
                """)
            
            gr.Examples(
                examples=[
                    ["ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ", "Hybrid", 5, True],
                    ["æœºå™¨å­¦ä¹ çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ", "Hybrid", 5, True],
                    ["æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ", "FAISS", 3, True],
                    ["ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ", "BM25", 5, True],
                    ["å¦‚ä½•åšçº¢çƒ§è‚‰ï¼Ÿ", "Hybrid", 5, True],  # æ•…æ„çš„å¤±è´¥æ¡ˆä¾‹
                ],
                inputs=[
                    question_input,
                    retriever_type,
                    top_k_slider,
                    show_sources_checkbox
                ],
                label="ç‚¹å‡»åŠ è½½ç¤ºä¾‹"
            )
        
        # Tab 3: æ£€ç´¢å™¨å¯¹æ¯”
        with gr.Tab("ğŸ”„ æ£€ç´¢å™¨å¯¹æ¯”"):
            gr.Markdown("""
            ## ğŸ“Š å¯¹æ¯”ä¸åŒæ£€ç´¢å™¨çš„æ€§èƒ½
            
            åŒæ—¶ä½¿ç”¨ **FAISS**ã€**BM25** å’Œ **Hybrid** ä¸‰ç§æ£€ç´¢å™¨ï¼ŒæŸ¥çœ‹å®ƒä»¬çš„æ£€ç´¢ç»“æœå·®å¼‚ã€‚
            """)
            
            with gr.Row():
                with gr.Column(scale=1):
                    compare_question = gr.Textbox(
                        label="â“ è¾“å…¥é—®é¢˜",
                        placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ",
                        lines=3
                    )
                    
                    compare_top_k = gr.Slider(
                        label="ğŸ“Š Top-K",
                        minimum=1,
                        maximum=10,
                        value=3,
                        step=1
                    )
                    
                    compare_button = gr.Button(
                        "ğŸ” å¼€å§‹å¯¹æ¯”",
                        variant="primary",
                        size="lg"
                    )
                
                with gr.Column(scale=2):
                    comparison_output = gr.Markdown()
            
            with gr.Accordion("ğŸ“Š å¯¹æ¯”æ•°æ®è¡¨æ ¼", open=True):
                comparison_table = gr.Dataframe(
                    headers=["æ£€ç´¢å™¨", "æ’å", "å†…å®¹é¢„è§ˆ", "ç›¸å…³åº¦"],
                    interactive=False
                )
            
            compare_button.click(
                fn=demo_app.compare_retrievers,
                inputs=[compare_question, compare_top_k],
                outputs=[comparison_output, comparison_table]
            )
        
        # Tab 4: æ€§èƒ½æµ‹è¯•
        with gr.Tab("âš¡ æ€§èƒ½æµ‹è¯•"):
            gr.Markdown("""
            ## ğŸ“ˆ æµ‹è¯•ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
            
            æµ‹é‡ä¸åŒæ£€ç´¢å™¨çš„ **å»¶è¿Ÿ** å’Œ **ååé‡**ï¼Œè¯„ä¼°ç³»ç»Ÿæ€§èƒ½ã€‚
            """)
            
            with gr.Row():
                with gr.Column(scale=1):
                    num_queries_input = gr.Slider(
                        label="ğŸ”¢ æµ‹è¯•æŸ¥è¯¢æ•°",
                        minimum=5,
                        maximum=50,
                        value=10,
                        step=5,
                        info="å»ºè®® 10-20 æ¬¡æŸ¥è¯¢"
                    )
                    
                    perf_test_button = gr.Button(
                        "ğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•",
                        variant="primary",
                        size="lg"
                    )
                
                with gr.Column(scale=2):
                    perf_output = gr.Markdown()
            
            with gr.Accordion("ğŸ“Š æ€§èƒ½æ•°æ®è¡¨æ ¼", open=True):
                perf_table = gr.Dataframe(
                    headers=["æ£€ç´¢å™¨", "å¹³å‡å»¶è¿Ÿ(ç§’)", "æœ€å°å»¶è¿Ÿ(ç§’)", "æœ€å¤§å»¶è¿Ÿ(ç§’)", "ååé‡(q/s)"],
                    interactive=False
                )
            
            perf_test_button.click(
                fn=demo_app.run_performance_test,
                inputs=[num_queries_input],
                outputs=[perf_output, perf_table]
            )
        
        # Tab 5: æŸ¥è¯¢å†å²
        with gr.Tab("ğŸ“œ æŸ¥è¯¢å†å²"):
            gr.Markdown("""
            ## ğŸ“Š æŸ¥çœ‹å†å²æŸ¥è¯¢è®°å½•
            
            è®°å½•äº†æ‰€æœ‰æŸ¥è¯¢çš„é—®é¢˜ã€ä½¿ç”¨çš„æ£€ç´¢å™¨ã€è€—æ—¶ç­‰ä¿¡æ¯ã€‚
            """)
            
            history_refresh_button = gr.Button("ğŸ”„ åˆ·æ–°å†å²", variant="secondary")
            history_table = gr.Dataframe(
                headers=["é—®é¢˜", "æ£€ç´¢å™¨", "è€—æ—¶(ç§’)", "ç­”æ¡ˆé¢„è§ˆ"],
                interactive=False
            )
            
            history_refresh_button.click(
                fn=demo_app.get_query_history,
                outputs=history_table
            )
        
        # Footer
        gr.Markdown("""
        ---
        
        ## ğŸ’¡ ä½¿ç”¨æŒ‡å—
        
        ### å¿«é€Ÿå¼€å§‹
        1. **åˆå§‹åŒ–ç³»ç»Ÿ**: åœ¨"ç³»ç»Ÿåˆå§‹åŒ–"æ ‡ç­¾é¡µé…ç½®å‚æ•°å¹¶åˆå§‹åŒ–
        2. **å¼€å§‹é—®ç­”**: åœ¨"æ™ºèƒ½é—®ç­”"æ ‡ç­¾é¡µè¾“å…¥é—®é¢˜
        3. **å¯¹æ¯”æ£€ç´¢å™¨**: åœ¨"æ£€ç´¢å™¨å¯¹æ¯”"æ ‡ç­¾é¡µæŸ¥çœ‹ä¸åŒæ£€ç´¢å™¨çš„æ•ˆæœ
        4. **æ€§èƒ½æµ‹è¯•**: åœ¨"æ€§èƒ½æµ‹è¯•"æ ‡ç­¾é¡µè¯„ä¼°ç³»ç»Ÿæ€§èƒ½
        
        ### æŠ€æœ¯æ ˆ
        - **æ¡†æ¶**: LangChain
        - **å‘é‡æ•°æ®åº“**: FAISS
        - **æ£€ç´¢å™¨**: FAISS å‘é‡æ£€ç´¢ã€BM25 æ–‡æœ¬æ£€ç´¢ã€æ··åˆæ£€ç´¢
        - **åµŒå…¥æ¨¡å‹**: BGE (Beijing Academy of Artificial Intelligence)
        - **è¯„æµ‹å·¥å…·**: RAGAS
        
        ### ç‰¹æ€§
        - âœ… å¤šç§æ–‡æ¡£æ ¼å¼æ”¯æŒ (PDFã€TXTã€DOCXã€XLSX)
        - âœ… æ™ºèƒ½æ–‡æ¡£åˆ†å—ä¸å‘é‡ç´¢å¼•
        - âœ… ä¸‰ç§æ£€ç´¢ç­–ç•¥å¯é€‰
        - âœ… å¸¦å¼•ç”¨çš„ç­”æ¡ˆç”Ÿæˆ
        - âœ… å®Œæ•´çš„æ€§èƒ½è¯„ä¼°ä½“ç³»
        - âœ… å®æ—¶æŸ¥è¯¢å†å²è®°å½•
        
        ### ğŸ“š ç›¸å…³èµ„æº
        - [LangChain æ–‡æ¡£](https://python.langchain.com/)
        - [RAGAS è¯„æµ‹æ¡†æ¶](https://github.com/explodinggradients/ragas)
        - [FAISS å‘é‡æ£€ç´¢](https://github.com/facebookresearch/faiss)
        
        ---
        
        <div style="text-align: center; color: #666; padding: 20px;">
            <p>ğŸŒŸ å¦‚æœè§‰å¾—æœ‰ç”¨ï¼Œè¯·ç»™é¡¹ç›®ä¸€ä¸ª Starï¼</p>
            <p>ğŸ’¬ æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Ÿæ¬¢è¿æ Issueï¼</p>
        </div>
        """)
    
    return demo


if __name__ == "__main__":
    # å¯åŠ¨å¢å¼ºç‰ˆ Web åº”ç”¨
    demo = create_enhanced_web_interface()
    
    # å°è¯•å¤šä¸ªç«¯å£ï¼Œé¿å…ç«¯å£å ç”¨é—®é¢˜
    for port in [7860, 7861, 7862, 7863, 7864]:
        try:
            print(f"\nå°è¯•åœ¨ç«¯å£ {port} å¯åŠ¨...")
            demo.launch(
                server_name="0.0.0.0",
                server_port=port,
                share=False,
                show_error=True
            )
            break  # æˆåŠŸå¯åŠ¨åˆ™é€€å‡ºå¾ªç¯
        except OSError as e:
            if "10048" in str(e) or "Cannot find empty port" in str(e):
                print(f"ç«¯å£ {port} å·²è¢«å ç”¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç«¯å£...")
                continue
            else:
                raise  # å…¶ä»–é”™è¯¯åˆ™æŠ›å‡º
    else:
        print("\nâŒ é”™è¯¯: æ— æ³•æ‰¾åˆ°å¯ç”¨ç«¯å£")
        print("è¯·æ‰‹åŠ¨å…³é—­å ç”¨ç«¯å£çš„è¿›ç¨‹æˆ–é‡å¯ç”µè„‘")

